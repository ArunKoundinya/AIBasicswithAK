<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>&lt;img src=&#39;https://arunkoundinya.github.io/AIBasicswithAK/logo.png&#39; style=&#39;width: 2.2em; height: 2.2em; border-radius: 30%;&#39;&gt; &lt;b&gt;AI Basics with AK&lt;/b&gt;</title>
<link>https://arunkoundinya.github.io/AIBasicswithAK/blogs/</link>
<atom:link href="https://arunkoundinya.github.io/AIBasicswithAK/blogs/index.xml" rel="self" type="application/rss+xml"/>
<description>LETS BREATHE DATA</description>
<generator>quarto-1.4.550</generator>
<lastBuildDate>Wed, 10 Apr 2024 04:00:00 GMT</lastBuildDate>
<item>
  <title>What metrics to be chosen for classification modelling?</title>
  <dc:creator>Arun Koundinya Parasa</dc:creator>
  <link>https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/</link>
  <description><![CDATA[ 





<p>This blog is a response to my teammates’ questions on <code>WhatsApp</code>. I’m sharing what I learned and my views on each question in a separate blog.</p>
<p>The biggest challenge in classification modelling is choosing the right metric for our problem. Adding to this, the confusion matrix creates more confusion for us as it has more metrics than we imagine.</p>
<p>Let’s cut short this, Arun. Ask for <code>GenAI</code> tools; we rely on them.</p>
<p>Go through this <code>ChatGPT</code> link <a href="https://chat.openai.com/share/44e15e4f-ed46-49e4-97c7-232d98a3006c">response</a>. It will not help. Why?</p>
<p>Arun! prompt has to be more specific.</p>
<p>Okay! Okay! Here is a revised prompt <a href="https://chat.openai.com/share/353267af-3dc4-449b-b0a1-2c4124a08ffe">response</a> from <code>ChatGPT</code>. Still, It doesn’t help. Why?</p>
<p>It is because we, humans, need to decide what the right metric is. For this, <code>Data Scientists</code> need to step up and do three essential things which are usually not in their comfort zone:</p>
<ul>
<li>Speak to business stakeholders and understand their purpose again.</li>
<li>Also, understand how they will use our predictions.</li>
<li>Finally, before jumping into modelling and its coding, check whether the dependent variable is defined per the business purpose and its corresponding usage.</li>
</ul>
<p>Step three is the recipe for experience, i.e., multiple tries and failures, which this blog will not discuss.</p>
<section id="confusion-matrix" class="level2">
<h2 class="anchored" data-anchor-id="confusion-matrix">1. Confusion Matrix</h2>
<p>Understanding the details of the <code>Confusion Matrix</code> is fundamental.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/confusion.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="857"></p>
<figcaption>Source : https://en.wikipedia.org/wiki/Confusion_matrix</figcaption>
</figure>
</div>
<p>The top four boxes are the result of prediction vs actual. The remaining boxes are divided metrics based on the numbers.</p>
<p>At this point in this blog, let’s not delve into these multiple metrics, as there are many :).</p>
</section>
<section id="simpler-thumb-rules" class="level2">
<h2 class="anchored" data-anchor-id="simpler-thumb-rules">2. Simpler Thumb Rules</h2>
<p>The more straightforward thumb rule is to print the classification report from sci-kit Learn and use one of these metrics for our classification modelling. This is an excellent place to start, as most of the ML problems we solve commercially fall into these four metrics.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/classification_report_python.png" class="quarto-figure quarto-figure-center figure-img" width="400" height="147"></p>
<figcaption>Source : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html</figcaption>
</figure>
</div>
<p>In most classification problems, the <code>target</code> class would be either a <code>positive</code> or <code>negative</code>; under this content, let us review the intuition behind these metrics.</p>
<section id="accuracy" class="level3">
<h3 class="anchored" data-anchor-id="accuracy">2.1. Accuracy</h3>
<p>In more straightforward English, it is how accurate you are. If there are <code>50 positive</code> and <code>50 negative</code> instances, accuracy measures how many times the model is able to predict accurately the right instances out of these 100 original instances.</p>
<p>If the model can correctly predict <code>30 positive</code> and <code>50 negative</code> instances, it means that the model’s accuracy is 80 correctly predicted instances out of the total 100 cases.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/cm-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Using this metric is always a problem as there is a high chance of giving more accuracy under imbalanced class scenarios.</p>
<p>If there are <code>5 positive</code> instances and <code>95 negative</code> instances and model-1 can predict <code>0 positive</code> instances and <code>85 negative</code> instances, we say that model-1 has 85% accuracy. While model-2 can predict four positive instances and <code>79 negative</code> instances, we say that model-2 has 83% accuracy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/cm-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>When we look at the accuracy metric without having the context of our data, we may choose a poor model-1, which has higher accuracy.</p>
</section>
<section id="recall" class="level3">
<h3 class="anchored" data-anchor-id="recall">2.2. Recall:</h3>
<p>In more straightforward English, recall is how much your model can recall the scenario. In a binary classification model, we usually refer to recall to the positive class to ensure that the model recalls the maximum positive instances. It calculates the proportion of true positive predictions among all actual positive instances.</p>
<p>Returning to our earlier example, which model is better off if there are <code>5 positive</code> instances and <code>95 negative</code> instances for model-1 and model-2?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/cm-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Here, if we want to be sure that the number of predicted positive instances upon actual positive instances should be more, we will select model-2 instead of model-1.</p>
<p>Now, let’s go back to our first example again, which is that there are <code>50 positive</code> instances and <code>50 negative</code> instances, and model-1 is able to predict <code>15 positive</code> instances and <code>50 negative</code> instances. Here, model 1 has almost 65% accuracy and a recall of 30% for positive instances. Similarly, model 2 can predict <code>25 positive</code> and <code>50 negative</code> instances. Here, model 2 has almost 65% accuracy and a recall of 50% for positive cases.</p>
<p>Model 2 is better when we look into recall metrics. Isn’t it so?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/cm-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>But, when completing the entire confusion matrix, it might look like this: do you see something off? What is the catch?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/cm-4.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Model-1 doesn’t have any false alarms, which means the model-1 is more precise in predicting positive instances.</p>
</section>
<section id="precision" class="level3">
<h3 class="anchored" data-anchor-id="precision">2.3. Precision:</h3>
<p>In more straightforward English, precision is the degree to which the model is precise, like hitting a bull’s eye for positive instances. High precision signifies a low false positive rate. Mathemically it is like ratio of true positive predictions to the total positive predictions made by the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/cm-4.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Under this scenario, model 1 is more precise.</p>
</section>
<section id="f1-score" class="level3">
<h3 class="anchored" data-anchor-id="f1-score">2.4. F1-Score:</h3>
<p>F1-Score is the harmonic mean of precision and recall. It provides a balance between precision and recall, making it a useful metric when there’s an uneven class distribution. A high F1-Score indicates both high precision and high recall.</p>
<p>Usually, while developing model we stick to one metric.</p>
</section>
</section>
<section id="why-stick-to-one-metric" class="level2">
<h2 class="anchored" data-anchor-id="why-stick-to-one-metric">3. Why stick to one metric?</h2>
<p>Often, rather than sticking to one model, it is wise to prepare multiple models, optimize them using the development dataset, and use the test set for the final performance. So, while optimizing and evaluating the models, it is advisable to stick to one metric because we cannot have different barometers for each model.</p>
</section>
<section id="how-to-choose" class="level2">
<h2 class="anchored" data-anchor-id="how-to-choose">4. How to Choose</h2>
<p>This part of the blog is the prime Purpose, which depends upon the two questions that <code>Data Scientists</code> need to ask the business stakeholders i.e.; Purpose and usage. Purpose gives the revenue for the model, and usage tells us the cost.</p>
<p>How come?</p>
<p>Let’s dwell on some examples.</p>
<section id="example---cross-sell" class="level3">
<h3 class="anchored" data-anchor-id="example---cross-sell">4.1. Example - Cross-Sell</h3>
<p><strong>Purpose</strong> - Identify Customers who can buy <code>Product1</code>. <strong>Usage1</strong> - Digital Campaigns ( Machine to Human ) <strong>Usage2</strong> - Non-Digital Campaigns ( Human to Human )</p>
<p>If we look at the Purpose, we might think <code>accuracy</code> would be sufficient to predict whether a customer will buy a particular product. This is the usual de facto approach we think.</p>
<p>Now, depending on usage, we need to optimize the metric, as it also tells us the cost of the usage.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/Example-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Usage1 -&gt; Usually, digital campaigns are low-cost. We want to be sure to reach as many right customers as possible at a low cost. So, in this scenario, it is better to optimize the models using <code>recall</code> since we don’t want to miss the right customer.</p>
<p>Usage2 -&gt; Usually, in non-digital campaigns, the cost is higher, and since the interaction is between humans, we have to ensure the trust chain of the model carries forward to the field team. So, in this scenario, it is better to optimize models using the <code>precision</code> metric.</p>
</section>
<section id="example---customer-attrition" class="level3">
<h3 class="anchored" data-anchor-id="example---customer-attrition">4.2. Example - Customer Attrition</h3>
<p><strong>Purpose</strong> - Identify Customers who would stop transacting <strong>Usage1</strong> - Carrot Communication in Digital way ( Machine to Human ) - Incentive <strong>Usage2</strong> - Carrot Communication in Non-Digital way ( Human to Human ) - Incentive</p>
<p>If we look at the Purpose, we might think <code>accuracy</code> would be sufficient as we just want to predict whether a customer would at-rite. This is the usual de facto approach we would think.</p>
<p>Now, depending on usage, let’s optimize the metric.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/Example-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Usage1 -&gt; Although digital campaigns are low-cost here, we need to ensure we predict all customers who will at-rite and give-away fewer incentives to the customers who actually don’t at-rite. So, in this scenario, it is better to optimize the models using the <code>F1-Score</code> metric.</p>
<p>Usage2 -&gt; Since trust-chain matters a lot in non-digital campaigns, apart from cost, it is better to optimize models using the <code>precision</code> metric.</p>
</section>
<section id="example---fraudulent-transaction" class="level3">
<h3 class="anchored" data-anchor-id="example---fraudulent-transaction">4.3. Example - Fraudulent Transaction</h3>
<p><strong>Purpose</strong> - Identify Customers’ accounts of fraudulent activity. <strong>Usage1</strong> - Digital Alarm ( Machine to Human ) <strong>Usage2</strong> - Non-Digital Alarm ( Human to Human )</p>
<p>If we just look at the Purpose, we might think that by default, we will select the <code>Precision</code> metric to ensure that fraudulent activity is arrested.</p>
<p>Now, depending on usage, let’s optimize the metric.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/Example-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Usage1 -&gt; Since digital campaigns are low-cost here, we must ensure we predict all fraudulent activities with fewer false alarms. So, it is better to optimize the model with <code>F1-Score</code>.</p>
<p>Usage2 -&gt; Since cost and trust-chain until customer matters a lot in non-digital campaigns, it is better to optimize models using the <code>precision</code> metric. However, in some scenarios <code>F1-Score</code> can be used if it is digitally assisted.</p>
<p>So, to summarize, we need to optimize the model based on the chosen metric of Purpose and usage, which is highly based on the context and business usage.</p>
</section>
</section>
<section id="can-we-change-the-metric-if-model-perform-is-poor" class="level2">
<h2 class="anchored" data-anchor-id="can-we-change-the-metric-if-model-perform-is-poor">5. Can we change the metric if model perform is poor?</h2>
<p>If we are unable to make goals, it is never advisable to change our goalpost. We always need to understand whether the goal post design is correct and then practice to make goals.</p>
</section>
<section id="further-discussions" class="level2">
<h2 class="anchored" data-anchor-id="further-discussions">6. Further Discussions</h2>
<p>Macro-average and weighted-average are not discussed here, which are better suited for discussing classification models with multiple class outcomes instead of binary ones.</p>
<script src="https://giscus.app/client.js" data-repo="ArunKoundinya/AIBasicswithAK" data-repo-id="R_kgDOLXPVRw" data-category="General" data-category-id="DIC_kwDOLXPVR84CdeVx" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="purple_dark" data-lang="en" crossorigin="anonymous" async="">
</script>


</section>

 ]]></description>
  <category>classification</category>
  <category>modelling</category>
  <category>metrics</category>
  <category>binaryclassifcation</category>
  <guid>https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/</guid>
  <pubDate>Wed, 10 Apr 2024 04:00:00 GMT</pubDate>
  <media:content url="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/metrics-classification-model/confusion.png" medium="image" type="image/png" height="89" width="144"/>
</item>
<item>
  <title>Does Data Scientists need to learn programming?</title>
  <dc:creator>Arun Koundinya Parasa</dc:creator>
  <link>https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/learn-programming/</link>
  <description><![CDATA[ 





<p>In an era of AI-enabled applications, chats, and programming (<a href="https://www.cognition-labs.com/blog-posts">now with Devin</a>), many youngsters and new entrants in the data science field assume that learning programming isn’t needed as almost all codes are available either on stack overflow or ChatGPT.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/learn-programming/image.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="373"></p>
</figure>
</div>
<p>However, programming is not just a copy-and-paste activity. But it is an iterative process of four pieces:</p>
<ul>
<li>What to Code?</li>
<li>How to Code?</li>
<li>Test the Code?</li>
<li>Coding with-in limitations.</li>
</ul>
<p>Here, <code>What to Code</code> can be comprehended as an <code>Ideaation/Requirement Gathering and Planning</code> stage. Ideas can be either intrinsic or extrinsic, but the primary reward goes to both, i.e., the idea and proper planning. This is itself an intensive area where the know-how of coding is required; otherwise, the entire planning and further execution would fall into a fallacy.</p>
<p><code>How to Code</code> involves the <code>Coding the requirement, unit-testing and documentation</code> stage. Coding becomes an iterative step along with unit testing since our code changes should not hamper that part of the unit.</p>
<p><code>Test the Code</code> involves <code>Reviewing the code and completing the system-testing</code> stage. After we complete our code, it needs to be reviewed and refined by a senior programmer, ideally with an intention to optimize its performance and ensure that this particular fix will not hamper other units in the system.</p>
<p><code>Coding within limitation</code> involves the <code>Deployment and Maintenance</code> stage. After the entire coding, testing, and documentation are completed, Deployment becomes a central learning as initial programming doesn’t involve physical limitations. Deployment limitations, becomes an iterative process to enhance our coding.</p>
<p>If we understand this complete cycle of programming, we will learn that coding is not just a copy-paste job but often requires a thought process for maintenance of the code for long-term usage. However, there might be some applications ( we can term such applications as <code>AI-assisted</code> programming applications ) in the future that would help automate some of these. Still, it would require a keen human eye and thought process for thorough Deployment.</p>
<p>If you heard about Scrum masters or Scrum certification it is good else google it. These people are often valued because they are aware of the fundamentals of agile methodological practices, which involve iterative steps of planning, coding, testing, and maintenance.</p>
<section id="some-future-or-current-technologies" class="level5">
<h5 class="anchored" data-anchor-id="some-future-or-current-technologies">Some Future or Current Technologies</h5>
<ul>
<li>Coding Assistant:
<ul>
<li>Code Autocompletion ; Example: Github Copilot</li>
<li>AI Driven Automation Testing; Example: Testim</li>
</ul></li>
<li>Minimal Coding:
<ul>
<li>Low-Code Platform ; Example: Microsoft Power Apps</li>
<li>No-Code Platform; Example: Bubble, Google AppSheet</li>
</ul></li>
<li>Power Computing:
<ul>
<li>Serverless Computing ; Example: AWS Lambda</li>
<li>Edge Computing; Example: AWS IoT</li>
<li>Quantum Computing; Example: IBM Quantum</li>
</ul></li>
</ul>
<p>View them as assistants that will fasten our process such that a particular idea’s “time to market” will come down exponentially.</p>
<script src="https://giscus.app/client.js" data-repo="ArunKoundinya/AIBasicswithAK" data-repo-id="R_kgDOLXPVRw" data-category="General" data-category-id="DIC_kwDOLXPVR84CdeVx" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="purple_dark" data-lang="en" crossorigin="anonymous" async="">
</script>


</section>

 ]]></description>
  <category>programming</category>
  <guid>https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/learn-programming/</guid>
  <pubDate>Sun, 17 Mar 2024 04:00:00 GMT</pubDate>
  <media:content url="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/learn-programming/image.png" medium="image" type="image/png" height="123" width="144"/>
</item>
<item>
  <title>Does Data Scientists need to understand Domain Knowledge?</title>
  <dc:creator>Arun Koundinya Parasa</dc:creator>
  <link>https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/is-domain-knowledge-important/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/is-domain-knowledge-important/image.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="439"></p>
</figure>
</div>
<p>This article intends to explain domain knowledge’s importance to freshers. The example provided below is from my own experience.</p>
<p>Businesses, by nature, want to earn profits, and why is it needed to make profits? From an employee perspective, the company gives salary and growth; from a company perspective, it intends to stay long, live and fresh as it grows.</p>
<p>Profit = Revenue - Cost</p>
<p>If the business needs to stay longer, the profit has to be on the positive side. It means either the revenue should increase or the cost should decrease; also, both can happen.</p>
<p>Let us explore simple business case scenarios that will help us learn why understanding the business comes first before we solutionize.</p>
<p>Case { Retail Store Business }</p>
<p>Assume that a data scientist was tasked to help the retail store stock the “right” quantity. By intuition, most “data scientists” consider the last X years of data for prediction. They may take various models from “simple averages” to “ARIMA” to “LSTM” and choose the best model based on the evaluation metrics like AIC or BIC, or MAPE. { Classic Time series modelling algorithms }</p>
<p>After this exercise, we will determine the quantity that could sell for the next period. Does it end here?</p>
<p>It would not, because we haven’t answered the business question of what should be the stock holding for the next period. i.e., how many goods should be stocked in the store?</p>
<p>Say, if we want to proceed with the model’s output itself. Since the model has inherent bias and variance, certain goods might get stocked out sooner, and others will not sell as predicted. { Alternatively, at times, the business could not procure the goods as indicated due to the cash flow constraint. Which we will discuss in other cases}</p>
<p>What happens if there is stock out; revenue drops. Alternatively, what happens if there is excess stock; cost increases. In both of these cases, the profit decreases.</p>
<p>Should we conclude that data science is not helping in this scenario? No, it isn’t.</p>
<p>It is just that it has not been adequately solutionized. So far, we have put the hat of “Machine Learning”, which needs to be started with the “Traditional Research” cap.</p>
<p>Under Traditional research, we may learn the following things: - Stock will come from warehouses every two days, for which the indent has to be shared a day earlier.</p>
<ul>
<li>In all case scenarios, a buffer or safety stock should be kept in the store for fast-moving items to avoid supply chain shocks.</li>
<li>Evading bull-whip effect, the probable forecast for certain goods must be shared with vendors for effective coordination.</li>
<li>A simulation must be constructed such that the stock at the store is not too high to avoid high business costs. Also, the store needs to maintain sufficient service levels(of the goods, i.e., stock availability) so the customer doesn’t face a bad experience.</li>
</ul>
<p>Based on these learnings, we will research any existing methodologies available to address these questions. Bingo!!</p>
<p>We would crack the solution by browsing the topics of Supply Chain Analytics or Operations Optimization.</p>
<p>Which will provide methods to arrive at Safety Stock, Re-order Level, Re-order Quantity and Service Levels.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/is-domain-knowledge-important/safety_stock.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="437"></p>
</figure>
</div>
<p>From here, we can use ML ( Time Series Modelling ) to arrive at tomorrow’s prediction and perform a simulation ( Monte Carlo Simulation ) to arrive at service levels for different clusters ( K-Means Clustering ) of products. These outputs are fed into the above solution we saw in “Traditional Research”.</p>
<p>Later we wear the “Software Development” hat to fine-tune the coding and keep the end-to-end pieces in hands-off mode.</p>
<p>In this way, Data Scientists must change hats to complete the work.</p>
<p>In this case, we can clearly see that the “Traditional Research” hat is hugely needed to understand the “lay of the land”. Else the entire modelling effort would have faced an identity crisis.</p>
<p>Is there any defined prescription or rules to understand the lay of the land? The answer is NO. The long answer is</p>
<ul>
<li>Curiosity to learn about the domain.</li>
<li>Asking the business the right questions.</li>
<li>Doing personal research as, at times, business is driven by perceptions or experience. { Only a “fresh eye” brings a new perspective } - Learning what others have done already. There would have been plenty of research already available. <span style="font-size: small;">{ There is no need for new IP in most of the scenarios }</span></li>
<li>Learning Consultant frameworks like BCG Matrix, Product Life Cycle, Customer Life Cycle, etc., will help us think on alternative perspectives.</li>
</ul>
<p>There are several examples that I have faced personally in my early data science career, which told me the hard way about the importance of domain knowledge. The above example is one such classic. Stay tuned for more Data science news.</p>
<script src="https://giscus.app/client.js" data-repo="ArunKoundinya/AIBasicswithAK" data-repo-id="R_kgDOLXPVRw" data-category="General" data-category-id="DIC_kwDOLXPVR84CdeVx" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="purple_dark" data-lang="en" crossorigin="anonymous" async="">
</script>



 ]]></description>
  <category>domain</category>
  <category>operations</category>
  <guid>https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/is-domain-knowledge-important/</guid>
  <pubDate>Tue, 25 Jul 2023 04:00:00 GMT</pubDate>
  <media:content url="https://arunkoundinya.github.io/AIBasicswithAK/blogs/posts/is-domain-knowledge-important/image.png" medium="image" type="image/png" height="131" width="144"/>
</item>
</channel>
</rss>
