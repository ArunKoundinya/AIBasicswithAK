[
  {
    "objectID": "blogs/posts/metrics-classification-model/index.html",
    "href": "blogs/posts/metrics-classification-model/index.html",
    "title": "What metrics to be chosen for classification modelling?",
    "section": "",
    "text": "This blog is a response to my teammates’ questions on WhatsApp. I’m sharing what I learned and my views on each question in a separate blog.\nThe biggest challenge in classification modelling is choosing the right metric for our problem. Adding to this, the confusion matrix creates more confusion for us as it has more metrics than we imagine.\nLet’s cut short this, Arun. Ask for GenAI tools; we rely on them.\nGo through this ChatGPT link response. It will not help. Why?\nArun! prompt has to be more specific.\nOkay! Okay! Here is a revised prompt response from ChatGPT. Still, It doesn’t help. Why?\nIt is because we, humans, need to decide what the right metric is. For this, Data Scientists need to step up and do three essential things which are usually not in their comfort zone:\nStep three is the recipe for experience, i.e., multiple tries and failures, which this blog will not discuss."
  },
  {
    "objectID": "blogs/posts/metrics-classification-model/index.html#confusion-matrix",
    "href": "blogs/posts/metrics-classification-model/index.html#confusion-matrix",
    "title": "What metrics to be chosen for classification modelling?",
    "section": "1. Confusion Matrix",
    "text": "1. Confusion Matrix\nUnderstanding the details of the Confusion Matrix is fundamental.\n\n\n\nSource : https://en.wikipedia.org/wiki/Confusion_matrix\n\n\nThe top four boxes are the result of prediction vs actual. The remaining boxes are divided metrics based on the numbers.\nAt this point in this blog, let’s not delve into these multiple metrics, as there are many :)."
  },
  {
    "objectID": "blogs/posts/metrics-classification-model/index.html#simpler-thumb-rules",
    "href": "blogs/posts/metrics-classification-model/index.html#simpler-thumb-rules",
    "title": "What metrics to be chosen for classification modelling?",
    "section": "2. Simpler Thumb Rules",
    "text": "2. Simpler Thumb Rules\nThe more straightforward thumb rule is to print the classification report from sci-kit Learn and use one of these metrics for our classification modelling. This is an excellent place to start, as most of the ML problems we solve commercially fall into these four metrics.\n\n\n\nSource : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n\n\nIn most classification problems, the target class would be either a positive or negative; under this content, let us review the intuition behind these metrics.\n\n2.1. Accuracy\nIn more straightforward English, it is how accurate you are. If there are 50 positive and 50 negative instances, accuracy measures how many times the model is able to predict accurately the right instances out of these 100 original instances.\nIf the model can correctly predict 30 positive and 50 negative instances, it means that the model’s accuracy is 80 correctly predicted instances out of the total 100 cases.\n\n\n\n\n\nUsing this metric is always a problem as there is a high chance of giving more accuracy under imbalanced class scenarios.\nIf there are 5 positive instances and 95 negative instances and model-1 can predict 0 positive instances and 85 negative instances, we say that model-1 has 85% accuracy. While model-2 can predict four positive instances and 79 negative instances, we say that model-2 has 83% accuracy.\n\n\n\n\n\nWhen we look at the accuracy metric without having the context of our data, we may choose a poor model-1, which has higher accuracy.\n\n\n2.2. Recall:\nIn more straightforward English, recall is how much your model can recall the scenario. In a binary classification model, we usually refer to recall to the positive class to ensure that the model recalls the maximum positive instances. It calculates the proportion of true positive predictions among all actual positive instances.\nReturning to our earlier example, which model is better off if there are 5 positive instances and 95 negative instances for model-1 and model-2?\n\n\n\n\n\nHere, if we want to be sure that the number of predicted positive instances upon actual positive instances should be more, we will select model-2 instead of model-1.\nNow, let’s go back to our first example again, which is that there are 50 positive instances and 50 negative instances, and model-1 is able to predict 15 positive instances and 50 negative instances. Here, model 1 has almost 65% accuracy and a recall of 30% for positive instances. Similarly, model 2 can predict 25 positive and 50 negative instances. Here, model 2 has almost 65% accuracy and a recall of 50% for positive cases.\nModel 2 is better when we look into recall metrics. Isn’t it so?\n\n\n\n\n\nBut, when completing the entire confusion matrix, it might look like this: do you see something off? What is the catch?\n\n\n\n\n\nModel-1 doesn’t have any false alarms, which means the model-1 is more precise in predicting positive instances.\n\n\n2.3. Precision:\nIn more straightforward English, precision is the degree to which the model is precise, like hitting a bull’s eye for positive instances. High precision signifies a low false positive rate. Mathemically it is like ratio of true positive predictions to the total positive predictions made by the model.\n\n\n\n\n\nUnder this scenario, model 1 is more precise.\n\n\n2.4. F1-Score:\nF1-Score is the harmonic mean of precision and recall. It provides a balance between precision and recall, making it a useful metric when there’s an uneven class distribution. A high F1-Score indicates both high precision and high recall.\nUsually, while developing model we stick to one metric."
  },
  {
    "objectID": "blogs/posts/metrics-classification-model/index.html#why-stick-to-one-metric",
    "href": "blogs/posts/metrics-classification-model/index.html#why-stick-to-one-metric",
    "title": "What metrics to be chosen for classification modelling?",
    "section": "3. Why stick to one metric?",
    "text": "3. Why stick to one metric?\nOften, rather than sticking to one model, it is wise to prepare multiple models, optimize them using the development dataset, and use the test set for the final performance. So, while optimizing and evaluating the models, it is advisable to stick to one metric because we cannot have different barometers for each model."
  },
  {
    "objectID": "blogs/posts/metrics-classification-model/index.html#how-to-choose",
    "href": "blogs/posts/metrics-classification-model/index.html#how-to-choose",
    "title": "What metrics to be chosen for classification modelling?",
    "section": "4. How to Choose",
    "text": "4. How to Choose\nThis part of the blog is the prime Purpose, which depends upon the two questions that Data Scientists need to ask the business stakeholders i.e.; Purpose and usage. Purpose gives the revenue for the model, and usage tells us the cost.\nHow come?\nLet’s dwell on some examples.\n\n4.1. Example - Cross-Sell\nPurpose - Identify Customers who can buy Product1. Usage1 - Digital Campaigns ( Machine to Human ) Usage2 - Non-Digital Campaigns ( Human to Human )\nIf we look at the Purpose, we might think accuracy would be sufficient to predict whether a customer will buy a particular product. This is the usual de facto approach we think.\nNow, depending on usage, we need to optimize the metric, as it also tells us the cost of the usage.\n\n\n\n\n\nUsage1 -&gt; Usually, digital campaigns are low-cost. We want to be sure to reach as many right customers as possible at a low cost. So, in this scenario, it is better to optimize the models using recall since we don’t want to miss the right customer.\nUsage2 -&gt; Usually, in non-digital campaigns, the cost is higher, and since the interaction is between humans, we have to ensure the trust chain of the model carries forward to the field team. So, in this scenario, it is better to optimize models using the precision metric.\n\n\n4.2. Example - Customer Attrition\nPurpose - Identify Customers who would stop transacting Usage1 - Carrot Communication in Digital way ( Machine to Human ) - Incentive Usage2 - Carrot Communication in Non-Digital way ( Human to Human ) - Incentive\nIf we look at the Purpose, we might think accuracy would be sufficient as we just want to predict whether a customer would at-rite. This is the usual de facto approach we would think.\nNow, depending on usage, let’s optimize the metric.\n\n\n\n\n\nUsage1 -&gt; Although digital campaigns are low-cost here, we need to ensure we predict all customers who will at-rite and give-away fewer incentives to the customers who actually don’t at-rite. So, in this scenario, it is better to optimize the models using the F1-Score metric.\nUsage2 -&gt; Since trust-chain matters a lot in non-digital campaigns, apart from cost, it is better to optimize models using the precision metric.\n\n\n4.3. Example - Fraudulent Transaction\nPurpose - Identify Customers’ accounts of fraudulent activity. Usage1 - Digital Alarm ( Machine to Human ) Usage2 - Non-Digital Alarm ( Human to Human )\nIf we just look at the Purpose, we might think that by default, we will select the Precision metric to ensure that fraudulent activity is arrested.\nNow, depending on usage, let’s optimize the metric.\n\n\n\n\n\nUsage1 -&gt; Since digital campaigns are low-cost here, we must ensure we predict all fraudulent activities with fewer false alarms. So, it is better to optimize the model with F1-Score.\nUsage2 -&gt; Since cost and trust-chain until customer matters a lot in non-digital campaigns, it is better to optimize models using the precision metric. However, in some scenarios F1-Score can be used if it is digitally assisted.\nSo, to summarize, we need to optimize the model based on the chosen metric of Purpose and usage, which is highly based on the context and business usage."
  },
  {
    "objectID": "blogs/posts/metrics-classification-model/index.html#can-we-change-the-metric-if-model-perform-is-poor",
    "href": "blogs/posts/metrics-classification-model/index.html#can-we-change-the-metric-if-model-perform-is-poor",
    "title": "What metrics to be chosen for classification modelling?",
    "section": "5. Can we change the metric if model perform is poor?",
    "text": "5. Can we change the metric if model perform is poor?\nIf we are unable to make goals, it is never advisable to change our goalpost. We always need to understand whether the goal post design is correct and then practice to make goals."
  },
  {
    "objectID": "blogs/posts/metrics-classification-model/index.html#further-discussions",
    "href": "blogs/posts/metrics-classification-model/index.html#further-discussions",
    "title": "What metrics to be chosen for classification modelling?",
    "section": "6. Further Discussions",
    "text": "6. Further Discussions\nMacro-average and weighted-average are not discussed here, which are better suited for discussing classification models with multiple class outcomes instead of binary ones."
  },
  {
    "objectID": "blogs/posts/foundation-models-llm/index.html",
    "href": "blogs/posts/foundation-models-llm/index.html",
    "title": "Foundation Models - LLMs",
    "section": "",
    "text": "In an era of chatgpt-enabled applications, most of us would have come across a term called as foundation model. In this article we will go through the basics of foundation model with respect to Large Language Models\n\n\n\n\n\nIn this article we will go through these topics in simple language:\n\nWhat are Foundation Models?\nWhat are Large Language Models?\nFundamental Architecture inside LLM?\nTypes of LLMs.\n\n\nWhat are Foundation Models?\nIf you go to any Indian restaurant and order Chicken Tikka Masala, it arrives at your table in no time. But if you were to make the same dish from scratch, it would take much longer. Most people assume that restaurants prepare curries in advance and simply reheat them when an order comes in. However, that would be a waste of resources if there aren’t any orders for that particular dish.\nInstead, restaurants use a common base gravy for a wide range of curries—such as Onion-Tomato Gravy or Onion-Spinach-Ginger Gravy. When an order hits the kitchen, chefs take this foundation gravy, add the required proteins, herbs, and veggies, and create a distinct curry as per the order. This approach saves time, ensures consistency, and allows for quick adaptation to different recipes.\nNow, we can apply this analogy to our AI models.\nFoundation Models are like these base gravies. They are trained on vast amounts of data and serve as a starting point for a range of applications. Unlike a base gravy, where we choose common ingredients, foundation models are built by training on diverse and extensive datasets.\nFor example, in Natural Language Processing (NLP), which deals with human language and conversation, a foundation model is trained on massive amounts of text data available on the internet (while ensuring ethical considerations). The resulting model acts as a foundation for various NLP tasks, such as chatbots, translations, and text generation.\nJust like chefs fine-tune a base gravy to create different dishes, developers fine-tune foundation models to adapt them for specific use cases.\n\n\nWhat are Large Language Models?\nLarge Language Models is one such foundation model that is trained on vast textual data using Deep Learning to understand and generate human type of language.\nMuch like our foundation gravy, LLMs act as a foundation for language-based applications. Instead of training a model from scratch for every NLP task, we build on this pre-trained foundation model and fine-tune them for specific purposes.\n\n\nFundamental Architecture inside LLM?\nAt the heart of LLMs lies the Transformer architecture, introduced in the groundbreaking paper \"Attention is All You Need\" by Vaswani et al. This architecture revolutionized NLP as this model helps to understand context and relationships between words more effectively.\n\n\n\n\n\nThe original paper was developed for a language translation use case. The input is in one language, say English, and the output is in another language, say Hindi. The blue section represents the Encoder, while the red section represents the Decoder.\n\nThe Encoder is responsible for understanding the input language.\nThe Decoder then generates the output language by interpreting the encoded information.\n\nThis encoder-decoder framework laid the foundation for many modern NLP models, enabling efficient translation, text generation, and other complex language tasks.\n\n\nTypes of LLMs?\nSince the original encoder-decoder architecture was designed for translation tasks, it is not ideally suited for other tasks like text classification, topic selection, generative tasks, and more.\n\nEncoder Only LLM - Representation Models\nTo address this, a general-purpose model was introduced in 2018: Bidirectional Encoder Representations from Transformers (BERT). This model served as a foundational architecture for Language AI in the years to come.\n\nThe Encoder-only architecture focuses on understanding and representing language rather than generating it.\nIt learns the nuances and contextual representations of language effectively.\nIdeal for tasks that require deep language comprehension.\n\n\n\nDecoder Only LLM - Generative Models\nSimilar to the Encoder-only architecture, a Decoder-only architecture was proposed in 2018 for generative tasks. This model is known as Generative Pre-trained Transformer (GPT).\n\nThe Decoder-only architecture is designed to generate text based on a given prompt or input.\nIt is widely used for conversational AI, creative writing, and content generation.\n\nThus, these two foundational LLM architectures are broadly used for a variety of applications today.\n\n\n\nUse Cases.\nEncoder-Only LLMs (BERT and similar models):\n\nText classification\nSentiment analysis\nInformation retrieval\nQuestion answering\n\nDecoder-Only LLMs (GPT and similar models):\n\nText generation\nConversational AI (chatbots, virtual assistants)\nSummarization\n\n\n\nReferences\n\nFoundation_model - Wiki Page\nHands-On Large Language Models by Jay Alammar, Maarten Grootendorst"
  },
  {
    "objectID": "blogs/posts/is-domain-knowledge-important/index.html",
    "href": "blogs/posts/is-domain-knowledge-important/index.html",
    "title": "Does Data Scientists need to understand Domain Knowledge?",
    "section": "",
    "text": "This article intends to explain domain knowledge’s importance to freshers. The example provided below is from my own experience.\nBusinesses, by nature, want to earn profits, and why is it needed to make profits? From an employee perspective, the company gives salary and growth; from a company perspective, it intends to stay long, live and fresh as it grows.\nProfit = Revenue - Cost\nIf the business needs to stay longer, the profit has to be on the positive side. It means either the revenue should increase or the cost should decrease; also, both can happen.\nLet us explore simple business case scenarios that will help us learn why understanding the business comes first before we solutionize.\nCase { Retail Store Business }\nAssume that a data scientist was tasked to help the retail store stock the “right” quantity. By intuition, most “data scientists” consider the last X years of data for prediction. They may take various models from “simple averages” to “ARIMA” to “LSTM” and choose the best model based on the evaluation metrics like AIC or BIC, or MAPE. { Classic Time series modelling algorithms }\nAfter this exercise, we will determine the quantity that could sell for the next period. Does it end here?\nIt would not, because we haven’t answered the business question of what should be the stock holding for the next period. i.e., how many goods should be stocked in the store?\nSay, if we want to proceed with the model’s output itself. Since the model has inherent bias and variance, certain goods might get stocked out sooner, and others will not sell as predicted. { Alternatively, at times, the business could not procure the goods as indicated due to the cash flow constraint. Which we will discuss in other cases}\nWhat happens if there is stock out; revenue drops. Alternatively, what happens if there is excess stock; cost increases. In both of these cases, the profit decreases.\nShould we conclude that data science is not helping in this scenario? No, it isn’t.\nIt is just that it has not been adequately solutionized. So far, we have put the hat of “Machine Learning”, which needs to be started with the “Traditional Research” cap.\nUnder Traditional research, we may learn the following things: - Stock will come from warehouses every two days, for which the indent has to be shared a day earlier.\n\nIn all case scenarios, a buffer or safety stock should be kept in the store for fast-moving items to avoid supply chain shocks.\nEvading bull-whip effect, the probable forecast for certain goods must be shared with vendors for effective coordination.\nA simulation must be constructed such that the stock at the store is not too high to avoid high business costs. Also, the store needs to maintain sufficient service levels(of the goods, i.e., stock availability) so the customer doesn’t face a bad experience.\n\nBased on these learnings, we will research any existing methodologies available to address these questions. Bingo!!\nWe would crack the solution by browsing the topics of Supply Chain Analytics or Operations Optimization.\nWhich will provide methods to arrive at Safety Stock, Re-order Level, Re-order Quantity and Service Levels.\n\n\n\n\n\nFrom here, we can use ML ( Time Series Modelling ) to arrive at tomorrow’s prediction and perform a simulation ( Monte Carlo Simulation ) to arrive at service levels for different clusters ( K-Means Clustering ) of products. These outputs are fed into the above solution we saw in “Traditional Research”.\nLater we wear the “Software Development” hat to fine-tune the coding and keep the end-to-end pieces in hands-off mode.\nIn this way, Data Scientists must change hats to complete the work.\nIn this case, we can clearly see that the “Traditional Research” hat is hugely needed to understand the “lay of the land”. Else the entire modelling effort would have faced an identity crisis.\nIs there any defined prescription or rules to understand the lay of the land? The answer is NO. The long answer is\n\nCuriosity to learn about the domain.\nAsking the business the right questions.\nDoing personal research as, at times, business is driven by perceptions or experience. { Only a “fresh eye” brings a new perspective } - Learning what others have done already. There would have been plenty of research already available. { There is no need for new IP in most of the scenarios }\nLearning Consultant frameworks like BCG Matrix, Product Life Cycle, Customer Life Cycle, etc., will help us think on alternative perspectives.\n\nThere are several examples that I have faced personally in my early data science career, which told me the hard way about the importance of domain knowledge. The above example is one such classic. Stay tuned for more Data science news."
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode04/homework.html",
    "href": "AI_Vlogs/homework/season02/episode04/homework.html",
    "title": "Home Work 04",
    "section": "",
    "text": "This is our fourth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - Why class character is not available in python\nUse comments in py file or markdown in ipynb file to provide the answer\n\n\nQuestion 2 - Performing different creative versions of slicings and indexing.\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode12/homework.html",
    "href": "AI_Vlogs/homework/season02/episode12/homework.html",
    "title": "Home Work 12",
    "section": "",
    "text": "This is our twelveth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - Rewrite the following function using a lambda function to make it more concise:\n\ndef multiply_by_two(x):\n    return x * 2\n\n\n\nQuestion 2\nConsider the list numbers = [1, 2, 3, 4, 5]. Using Lambda functions cube of squareroot of these numbers to a list.\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode05/homework.html",
    "href": "AI_Vlogs/homework/season02/episode05/homework.html",
    "title": "Home Work 05",
    "section": "",
    "text": "This is our fifth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - What are the limitations of the string operators that we have learned in this week. Please pick atleast two\nUse comments in py file or markdown in ipynb file to provide the answer\n\n\nQuestion 2 - Complete the following subsquestions.\n\nvariable = \"  Python is a versatile and powerful programming language used for web development, data analysis, artificial intelligence, and much more !:)\"\n\nQuestion 2.1 - Remove the trailing spaces and special characters\n\nQuestion 2.2 - Split the sentences by comma\n\nQuestion 2.3 - Pick one split in the above sentence and replace any text of your choice to your name\n\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode02/homework.html",
    "href": "AI_Vlogs/homework/season02/episode02/homework.html",
    "title": "Home Work 02",
    "section": "",
    "text": "This is our second homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - What is Syntax Error? Provide some examples of Syntax Error?\nUse comments in py file or markdown in ipynb file to provide the answer\n\n\nQuestion 2 - What is meant by Mutable data type? How is it useful in programming?\nHint: Think from Data Science perspective.\nUse comments in py file or markdown in ipynb file to provide the answer\n\n\nQuestion 3 - Create a function such that you can initialize the variable as zero.\nCheck slides for reference of week1 and week2.\n\n\nQuestion 4 - How to convert from one data type to other data type and store in a variable\nHint: Google how to change the data type of a variable in python\nUse comments in py file or markdown in ipynb file to provide the answer\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode16/homework.html",
    "href": "AI_Vlogs/homework/season02/episode16/homework.html",
    "title": "Home Work 14",
    "section": "",
    "text": "This is our fourteenth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 2 Replicate the below figure\n\n\n\n\n\n{ Referred from https://www.linkedin.com/pulse/data-science-interview-question-oop-inheritance-onyshchenko-phd }\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode11/homework.html",
    "href": "AI_Vlogs/homework/season02/episode11/homework.html",
    "title": "Home Work 11",
    "section": "",
    "text": "This is our eleventh homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 -\nWrite a Python function called calculate_discount_price that takes two parameters: price and discount_rate. The function should calculate the discounted price based on the given price and discount rate, and return the discounted price.\n\n\nQuestion 2\nCreate a Python function called generate_operations that takes two numbers a and b as parameters. Inside generate_operations, define an inner function called perform_operations that takes the same parameters a and b, and returns the result of three operations: addition, subtraction, and multiplication of a and b. Finally, the generate_operations function should return the result obtained from the perform_operations function.\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode01/homework.html",
    "href": "AI_Vlogs/homework/season02/episode01/homework.html",
    "title": "Home Work 01",
    "section": "",
    "text": "This is an introductory homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - Create a Github Account\nRefer to this video if you have doubts.\n\n\n\nQuestion 2 - Install VS Code or Use Google Colab\nRefer to this video for installing VS Code\n\nRefer to this video for Google Colab Python File Creation.\n\n\n\nQuestion 3 - Create a Python File or Jupyter Notebook file that answers the following questions:\n\nCreate a function that adds two numbers.\nCreate a function that subtracts two numbers.\n\nCheck slides for reference.\n\n\nQuestion 3 - Create a GitHub Repository, upload the file, and share the link in the comment box for my review.\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode15/homework.html",
    "href": "AI_Vlogs/homework/season02/episode15/homework.html",
    "title": "Home Work 13",
    "section": "",
    "text": "This is our thirteenth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 Replicate the below figure\n\n\n\n\n\n{ Referred from https://www.geeksforgeeks.org/python-classes-and-objects/ }\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Basics with AK",
    "section": "",
    "text": "Welcome to my AI Vlog Series Hub! 🚀 This website is crafted to elevate your experience with my YouTube AI Vlog Series. Let’s dive into the exciting content structured into two sections:\n💻 Notes and Codes: Uncover the magic behind the scenes! Starting from Season-2 onward, immerse yourself in detailed notes and codes. May be presented in markdown files or dynamic presentations.\n📚 Assisted Blogs: Thoughtfully created blogs designed to complement the AI Vlog Series. It has fascinating details that bring the series to life.\nI’m thrilled to embark on this next chapter, and I hope you’ll find joy and enrichment in exploring these resources. Your support is my motivation—thank you for being a part of this exciting adventure! 🌟\nMy Last three videos"
  },
  {
    "objectID": "AI_Vlogs/index.html",
    "href": "AI_Vlogs/index.html",
    "title": "AI Basics with AK",
    "section": "",
    "text": "This format is inspired from my data visualization class; So will be sticking to this format.\n\n\n\n\n\n    \n    Season-2 : Introduction to Python ProgrammingSeason-2\n    \n\n\n\nEpisode\nEpisode Title\nSlides / Handout\nHome Work\nYouTube Video Link\nYouTube Demo File\nDate of Upload\n\n\n\nEpisode - 01\nWhy Python?\nLink\nLink\nLink\nNA\n1st March 2024\n\n\n\nEpisode - 02\nPython Data Types\nLink\nLink\nLink\nDemo File\n8th March 2024\n\n\n\nEpisode - 03\nNumbers & Booleans\nLink\nLink\nLink\nDemo File\n17th March 2024\n\n\n\nEpisode - 04\nStrings & Characters - 1\nLink\nLink\nLink\nNA\n24th March 2024\n\n\n\nEpisode - 05\nStrings & Characters - 2\nLink\nLink\nLink\nNA\n31st March 2024\n\n\n\nEpisode - 06\nStrings & Characters - 3\nLink\nLink\nLink\nDemo File\n7th April 2024\n\n\n\nEpisode - 07\nLoops and Breaks\nLink\nLink\nLink\nDemo File\n15th April 2024\n\n\n\nEpisode - 08\nLists & Tuples\nLink\nLink\nLink\nDemo File\n22nd April 2024\n\n\n\nEpisode - 09\nList Operations\nLink\nLink\nLink\nDemo File\n11th May 2024\n\n\n\nEpisode - 10\nDictionaries\nLink\nLink\nLink\nDemo File\n13th May 2024\n\n\n\nEpisode - 11\nFunctions\nLink\nLink\nLink\nDemo File\n17th May 2024\n\n\n\nEpisode - 12\nLambda Functions\nLink\nLink\nLink\nNA\n31st May 2024\n\n\n\nEpisode - 13\nLambda Functions with Map,Reduce & Filter\nLink\nNA\nLink\nDemo File\n17th June 2024\n\n\n\nEpisode - 14\nObject Orientied Programming\nLink\nNA\nLink\nNA\n21st July 2024\n\n\n\nEpisode - 15\nClasses & Objects\nLink\nLink\nLink\nDemo File\n28th July 2024\n\n\n\nEpisode - 16\nInheritance\nLink\nLink\nLink\nDemo File\n31st July 2024\n\n\n\nEpisode - 17\nMutiple Inheritance\nLink\nLink\nLink\nNA\n6th April 2025\n\n\n\nEpisode - 18\nMulti Threading & Multi Processing\nLink\nNA\nLink\nDemo File\n13th April 2025\n\n\n\nEpisode - 19\nDebugging the Code\nLink\nNA\nLink\nDemo File\n20th April 2025\n\n\n\nEpisode - 20\nEnd to End Project\nNA\nNA\nLink\nLink\n27th April 2025\n\n\n\nEpisode - 21\nDeploying the Project\nNA\nNA\nLink\nLink\nDemo File\n4th May 2025\n\n\n\n\n\n\n\n    \n    Season 03 - Introduction to StatisticsSeason-3\n    \n\n\n\nEpisode\nEpisode Title\nSlides / Handout\nHome Work\nYouTube Video Link\nYouTube Demo File\nDate of Upload\n\n\nEpisode - 01\nWhat is Statistics?\nLink\nNA\nLink\nNA\n30th May 2025\n\n\n\n\n\n\n\n    \n    Season-4Hide\n    \n\n\n\nNot Yet Available"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode17/homework.html",
    "href": "AI_Vlogs/homework/season02/episode17/homework.html",
    "title": "Home Work 15",
    "section": "",
    "text": "This is our fifteenth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nWhat is the difference between Single Inheritance, Multi-Level Inheritance, Multiple Inheritance and Multi-Path Inheritance?\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode09/homework.html",
    "href": "AI_Vlogs/homework/season02/episode09/homework.html",
    "title": "Home Work 09",
    "section": "",
    "text": "This is our nineth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - Consider a list named ‘numbers’ containing the elements [1, 2, 3, 4, 5]. Perform the following operations:\n\nAppend the number 6 to the end of the list using the append() method.\nRemove and return the last element of the list using the pop() method.\n\n\n\nQuestion 2 - Given a list ‘original’ with elements [1, 2, 3] and another list ‘additional’ with elements [4, 5, 6], perform the following:\n\nExtend the ‘original’ list by adding all elements from the ‘additional’ list using the extend() method.\nCreate a new list named ‘squared’ containing the squares of all numbers using list comprehensions.\n\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode07/homework.html",
    "href": "AI_Vlogs/homework/season02/episode07/homework.html",
    "title": "Home Work 07",
    "section": "",
    "text": "This is our seventh homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - Write a Program that takes integer as input and check if it positive , nevative or zero.\n\n\nQuestion 2 - Create a Factorial Output of same integer using both For Loop and While Loop.\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode06/homework.html",
    "href": "AI_Vlogs/homework/season02/episode06/homework.html",
    "title": "Home Work 06",
    "section": "",
    "text": "This is our sixth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - What are the limitations of the f-style operators.\nUse comments in py file or markdown in ipynb file to provide the answer\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode10/homework.html",
    "href": "AI_Vlogs/homework/season02/episode10/homework.html",
    "title": "Home Work 10",
    "section": "",
    "text": "This is our tenth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - Given the following dictionary inventory, perform the following operations and print the results:\n\ninventory = {'apples': 20, 'bananas': 15, 'oranges': 30}\n\n\nAdd 10 more apples to the inventory.\nRemove ‘oranges’ from the inventory.\nCheck if ‘bananas’ are present in the inventory.\nPrint the total number of items in the inventory.\nPrint a list of all fruits in the inventory.\n\n\n\nQuestion 2 - Create a dictionary comprehension that counts the occurrences of each letter in the word ‘banana’. Print the resulting dictionary.\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode03/homework.html",
    "href": "AI_Vlogs/homework/season02/episode03/homework.html",
    "title": "Home Work 03",
    "section": "",
    "text": "This is our third homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - Create the function for arthimetic operations\nFunction accepts two numbers and perform all the arthimetic operation\nOutput should atleast resemble like this\n\narthimetic_operations(10,3)\n\nAddition:  13\nSubtraction:  7\nMultiplication:  30\nDivision:  3.33\nRemainder:  1\nExponential: 1000\n\n\n\n\nQuestion 2 - What are the mistakes in the following code\nWhat is the reason for that error\n\na = 10\nb = 0\n\nprint(a / b)\n\nprint(a % b)\n\nprint(a = b)\n\nprint(a &gt; b)\n\nnew =  5._23\n\nprint(new + a)\n\nUse comments in py file or markdown in ipynb file to provide the answer\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "AI_Vlogs/homework/season02/episode08/homework.html",
    "href": "AI_Vlogs/homework/season02/episode08/homework.html",
    "title": "Home Work 08",
    "section": "",
    "text": "This is our eigth homework assignment where you will address the following questions and submit your GitHub link containing either a Jupyter Notebook or a Python file\n\nQuestion 1 - What is the difference between a list and a tuple in Python?\nUse comments in py file or markdown in ipynb file.\n\n\nQuestion 2 - Given a list of integers, find the sum of all the elements\nExample [1,2,3,4] answer should be 10\n———————————————— Happy Learning —————————————————–"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "My Views and learnings on AI",
    "section": "",
    "text": "Representation Models - Text Classification\n\n\n\nnlp\n\nllm\n\ndeeplearning\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nArun Koundinya Parasa\n\n\n\n\n\n\n\n\n\n\n\n\nFoundation Models - LLMs\n\n\n\nnlp\n\nllm\n\ndeeplearning\n\n\n\n\n\n\n\n\n\nFeb 15, 2025\n\n\nArun Koundinya Parasa\n\n\n\n\n\n\n\n\n\n\n\n\nWhat metrics to be chosen for classification modelling?\n\n\n\nclassification\n\nmodelling\n\nmetrics\n\nbinaryclassifcation\n\n\n\n\n\n\n\n\n\nApr 10, 2024\n\n\nArun Koundinya Parasa\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Data Scientists need to learn programming?\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nMar 17, 2024\n\n\nArun Koundinya Parasa\n\n\n\n\n\n\n\n\n\n\n\n\nDoes Data Scientists need to understand Domain Knowledge?\n\n\n\ndomain\n\noperations\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\nArun Koundinya Parasa\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/posts/learn-programming/index.html",
    "href": "blogs/posts/learn-programming/index.html",
    "title": "Does Data Scientists need to learn programming?",
    "section": "",
    "text": "In an era of AI-enabled applications, chats, and programming (now with Devin), many youngsters and new entrants in the data science field assume that learning programming isn’t needed as almost all codes are available either on stack overflow or ChatGPT.\n\n\n\n\n\nHowever, programming is not just a copy-and-paste activity. But it is an iterative process of four pieces:\n\nWhat to Code?\nHow to Code?\nTest the Code?\nCoding with-in limitations.\n\nHere, What to Code can be comprehended as an Ideaation/Requirement Gathering and Planning stage. Ideas can be either intrinsic or extrinsic, but the primary reward goes to both, i.e., the idea and proper planning. This is itself an intensive area where the know-how of coding is required; otherwise, the entire planning and further execution would fall into a fallacy.\nHow to Code involves the Coding the requirement, unit-testing and documentation stage. Coding becomes an iterative step along with unit testing since our code changes should not hamper that part of the unit.\nTest the Code involves Reviewing the code and completing the system-testing stage. After we complete our code, it needs to be reviewed and refined by a senior programmer, ideally with an intention to optimize its performance and ensure that this particular fix will not hamper other units in the system.\nCoding within limitation involves the Deployment and Maintenance stage. After the entire coding, testing, and documentation are completed, Deployment becomes a central learning as initial programming doesn’t involve physical limitations. Deployment limitations, becomes an iterative process to enhance our coding.\nIf we understand this complete cycle of programming, we will learn that coding is not just a copy-paste job but often requires a thought process for maintenance of the code for long-term usage. However, there might be some applications ( we can term such applications as AI-assisted programming applications ) in the future that would help automate some of these. Still, it would require a keen human eye and thought process for thorough Deployment.\nIf you heard about Scrum masters or Scrum certification it is good else google it. These people are often valued because they are aware of the fundamentals of agile methodological practices, which involve iterative steps of planning, coding, testing, and maintenance.\n\nSome Future or Current Technologies\n\nCoding Assistant:\n\nCode Autocompletion ; Example: Github Copilot\nAI Driven Automation Testing; Example: Testim\n\nMinimal Coding:\n\nLow-Code Platform ; Example: Microsoft Power Apps\nNo-Code Platform; Example: Bubble, Google AppSheet\n\nPower Computing:\n\nServerless Computing ; Example: AWS Lambda\nEdge Computing; Example: AWS IoT\nQuantum Computing; Example: IBM Quantum\n\n\nView them as assistants that will fasten our process such that a particular idea’s “time to market” will come down exponentially."
  },
  {
    "objectID": "blogs/posts/representation_models/index.html",
    "href": "blogs/posts/representation_models/index.html",
    "title": "Representation Models - Text Classification",
    "section": "",
    "text": "In the previous article, we explored the master chef analogy for foundation models. We also discussed how transformers serve as the basic architecture for large language models. Depending on which part of the transformer model is used — encoder or decoder— we have two types of LLMs: representation models, which represent words as numbers, and generative models, which generate text.\n\n\n\n\n\nIn this article we will go through these topics in simpler language:\n\nWhat are Reprensentation Models?\nWhat is BERT ?\nWhat is Sentence Transformers?\n\n\nWhat are Representation Models?\nImagine stepping into a vast library where every book is catalogued not just by its title but by the essence of its content. Representation models work in a similar way. They convert text—be it a word, a sentence, or a whole document—into a series of numbers, often referred to as vectors. These vectors serve as unique fingerprints, capturing the context, sentiment, and meaning embedded within the text.\nBy transforming text into these numerical fingerprints, representation models empower machines to perform tasks like text classification, sentiment analysis, and topic detection without needing to understand language in the traditional sense. In short, they provide a foundational layer that bridges raw text and actionable insights.\n\n\nWhat is BERT?\nBERT, which stands for Bidirectional Encoder Representations from Transformers, is one of the most influential models in NLP. Developed by Google, BERT revolutionized language understanding by taking context into account from both directions—left and right of each word.\n\n\n\n\n\nHere’s what makes BERT stand out:\n\nBidirectional Context: Unlike earlier models that read text in a single direction, BERT examines the entire sentence simultaneously, ensuring a richer and more nuanced understanding of word meanings.\nMassive Pre-training: BERT is trained on extensive datasets using innovative tasks like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), which allow it to learn intricate language patterns.\nVersatility: From text classification and sentiment analysis to question answering, BERT’s robust representations make it adaptable to a wide range of NLP tasks.\n\n\n\n\n\n\nThink of BERT as a master chef who not only knows every ingredient (word) in a recipe (sentence) but also understands how they all interact to create the perfect flavor (meaning).\n\n\nWhat is Sentence Transformers?\nWhile BERT provides deep contextual understanding at the word and sentence level, Sentence Transformers are tailored to generate fixed-size embeddings for entire sentences. These embeddings make it easier to compare, cluster, and classify text based on semantic similarity.\n\n\n\n\n\nKey aspects of Sentence Transformers include:\n\nPre-Training: Sentence Transformers are also called as SBERT is fine-tuned on sentence pairs using siamese architecture as show in above image.\nEfficient Semantic Comparison: By mapping sentences to a common vector space, these models enable quick and accurate comparisons—essential for tasks like semantic search and clustering.\nEnhanced Text Classification: Converting sentences into embeddings simplifies the process of sorting texts into categories, as similar sentences will have similar vector representations.\nReal-World Applications: From grouping similar customer reviews to organizing news articles, Sentence Transformers provide a robust framework for handling diverse text classification challenges.\n\nImagine Sentence Transformers as a refined culinary tool that not only identifies each ingredient but also captures the overall recipe. Ideally, Sentence Transformers create better embeddings since it captures the context of overall recipe rather than at ingredient levels.\n\n\nConclusion\nRepresentation models are the unsung heroes behind many NLP applications, transforming raw text into meaningful data that machines can understand. BERT and Sentence Transformers exemplify how far we’ve come—each offering unique advantages in capturing the subtle nuances of language. Whether it’s for text classification or broader language understanding tasks, these models continue to push the boundaries of what’s possible in AI.\n\n\nReferences\n\nTransformers in NLP: BERT and Sentence Transformers\nHands-On Large Language Models by Jay Alammar, Maarten Grootendorst\nChatGPT For Refining Language.\nBERT Text Classification using Keras\nIntroduction to Sentence Transformers"
  }
]