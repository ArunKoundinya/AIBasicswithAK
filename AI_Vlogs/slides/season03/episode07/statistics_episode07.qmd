---
title: "AI Basics with AK"
subtitle: "Season 03 - Introduction to Statistics"
format:
  revealjs:
    incremental: false
    auto-stretch: true
    slide-number: false
execute:
  echo: false
  warning: false
  message: false
---

# Episode 07 - Central Limit Theorem

## Re-Cap of Episode -05

::: {style="font-size: 70%;"}
| Term                 | Meaning                               |
|----------------------|---------------------------------------|
| **Sample Space (S)** | Set of all possible outcomes          |
| **Event (E)**        | A subset of the sample space          |
| **Outcome**          | A single result from the sample space |
:::

## Re-Cap of Episode -06

::: {style="font-size: 70%;"}
-   What if we roll many times? -\> After enough rolls -\> There is a pattern
:::

```{python}
import numpy as np
import plotly.graph_objects as go

np.random.seed(42)

# Define number of rolls for the slider
roll_sizes = [10, 50, 100, 500, 1000]

# Pre-generate die rolls for all sample sizes
die_rolls_dict = {n: np.random.randint(1, 7, n) for n in roll_sizes}

# Create figure
fig = go.Figure()

# Add traces for all roll sizes (initially only show default size, e.g., 100)
for i, n in enumerate(roll_sizes):
    visible = True if n == 100 else False
    fig.add_trace(go.Histogram(
        x=die_rolls_dict[n],
        xbins=dict(start=0.5, end=6.5, size=1),
        marker_color='skyblue',
        name=f'{n} rolls',
        visible=visible
    ))

# Create slider steps
steps = []
for i, n in enumerate(roll_sizes):
    step = dict(
        method="update",
        args=[{"visible": [False]*len(roll_sizes)}],
        label=f"{n} rolls"
    )
    step["args"][0]["visible"][i] = True
    steps.append(step)

sliders = [dict(
    active=2,  # default: 100 rolls
    currentvalue={"prefix": "Number of Rolls: "},
    pad={"t": 50},
    steps=steps
)]

# Update layout with slider
fig.update_layout(
    title="Rolling a Die: Frequency Histogram",
    xaxis_title="Die Face",
    yaxis_title="Frequency",
    xaxis=dict(tickmode='linear', tick0=1, dtick=1),
    sliders=sliders,
    showlegend=False
)

fig.show()


```

## Re-Cap of Episode -06

::: {style="font-size: 70%;"}
| Distribution | Type | Example | Shape / Key Feature |
|------------------|------------------|------------------|-------------------|
| Uniform | Discrete / Continuous | Rolling a die, random pick | All outcomes equally likely; flat histogram |
| Normal | Continuous | Heights, exam scores | Bell-shaped; most outcomes near the mean |
| Binomial | Discrete | Coin flips (count successes) | Counts of successes; hill-shaped at discrete points |
:::

## Re-Cap of Episode -06

```{python}
import numpy as np
import plotly.graph_objects as go

np.random.seed(42)

# Sample sizes and distributions
sample_sizes = [100, 500, 1000, 5000]
distributions = {
    "Normal": lambda n: np.random.normal(0, 1, n),
    "Uniform": lambda n: np.random.uniform(1, 6, n),
    "Binomial": lambda n: np.random.binomial(10, 0.5, n),
    }

# Create figure
fig = go.Figure()

# Generate all traces: distribution x sample_size
trace_indices = {}  # for dropdown visibility
for i, (dist_name, func) in enumerate(distributions.items()):
    trace_indices[dist_name] = []
    for j, size in enumerate(sample_sizes):
        visible = True if (dist_name == "Normal" and size == 1000) else False  # default selection
        fig.add_trace(go.Histogram(
            x=func(size),
            nbinsx=30,
            name=f"{dist_name} n={size}",
            histnorm="probability density",
            visible=visible
        ))
        trace_indices[dist_name].append(i * len(sample_sizes) + j)

# Create slider steps (sample sizes)
steps = []
for si, size in enumerate(sample_sizes):
    step = dict(
        method="update",
        args=[{"visible": [False]*len(fig.data)}],
        label=f"n={size}"
    )
    # Set visible True for the default distribution (Normal)
    for idx in trace_indices["Normal"]:
        if idx % len(sample_sizes) == si:
            step["args"][0]["visible"][idx] = True
    steps.append(step)

sliders = [dict(
    active=2,  # default n=1000
    currentvalue={"prefix": "Sample Size: "},
    pad={"t": 50},
    steps=steps
)]

# Create dropdown buttons for distributions
dropdown_buttons = []
for dist_name in distributions.keys():
    visibility = [False]*len(fig.data)
    for idx in trace_indices[dist_name]:
        # Set default visible sample_size=1000 (index 2)
        if idx % len(sample_sizes) == 2:
            visibility[idx] = True
    button = dict(
        label=dist_name,
        method="update",
        args=[{"visible": visibility}],
    )
    dropdown_buttons.append(button)

fig.update_layout(
    title="Probability Distribution",
    xaxis_title="Value",
    yaxis_title="Probability Density",
    showlegend=True,
    sliders=sliders,
    updatemenus=[dict(
        buttons=dropdown_buttons,
        direction="down",
        x=0.05,
        xanchor="left",
        y=1.15,
        yanchor="top"
    )]
)

fig.show()



```

## What did we see so far?

::::::: columns
:::: {.column width="50%"}
### A Very Natural Question

::: {style="font-size: 70%;"}
-   We saw Uniform distributions
-   We saw Binomial distributions
-   We saw Normal distributions

But real-world data is rarely perfectly normal

So why do statisticians keep using Gaussian assumptions?

This episode answers that exact question.
:::
::::

:::: {.column width="50%"}
### Important Clarification

::: {style="font-size: 70%;"}
-   If data is Uniform, it stays Uniform
-   If data is Skewed, it stays Skewed
-   Taking more samples does not change the population

Uniform data does NOT magically become normal.

This is critical to understand before CLT.
:::
::::
:::::::

## Population vs Sample & Usage in Practice

::::::: columns
:::: {.column width="50%"}
### Reality vs What We Actually Observe

::: {style="font-size: 70%;"}

-   Population → the true data-generating process
-   Sample → what we actually observe
-   Statistics → summaries we compute from samples

In practice:

-   We never see the full population
-   We rely on sample-based summaries
:::
::::

:::: {.column width="50%"}
### Central Tendency Metrics

::: {style="font-size: 70%;"}

In real analysis, we rarely use raw data directly.

We use:

-   Mean
-   Sum
-   Average score
-   Average loss
-   etc.;

These are aggregates, not raw observations.
:::
::::
:::::::

## CLT Is NOT About the Data - Instead it is a Statistic

::: {style="font-size: 70%;"}

-   CLT does not talk about individual data points

-   CLT talks about the distribution of the mean

Data can be Uniform. Mean of the data behaves differently

This distinction removes most confusion.
:::

## CLT Theorem

### Central Limit Theorem (Intuition)

::: {style="font-size: 70%;"}

If we:

-   Take independent samples

-   From any distribution (Uniform, Binomial, etc.)

-   With finite variance

Then:

> The distribution of the sample mean becomes approximately\
> Normal as sample size increases.
:::

## How CLT changes the behavior of aggregates

```{python}
import numpy as np
import plotly.graph_objects as go
import scipy.stats as stats
np.random.seed(42)

def generate_data(dist_type, n, seed=42):
    np.random.seed(seed)
    if dist_type == "Normal":
        return np.random.normal(loc=0, scale=1, size=n)
    elif dist_type == "Uniform":
        return np.random.uniform(low=-1, high=1, size=n)
    elif dist_type == "Exponential":
        return np.random.exponential(scale=1, size=n)
    else:
        return np.random.normal(loc=0, scale=1, size=n)

def calc_ci(data, confidence=0.95):
    n = len(data)
    mean = np.mean(data)
    sem = stats.sem(data)
    h = sem * stats.t.ppf((1 + confidence) / 2., n-1)
    return mean, mean - h, mean + h

# Sample sizes to precompute
sample_sizes = [5, 10, 20, 30, 50, 100, 200]
distributions = ['Normal', 'Uniform', 'Exponential']

fig = go.Figure()

# Store which trace indices correspond to which parameters
trace_info = []
trace_counter = 0

for dist in distributions:
    for n in sample_sizes:
        data = generate_data(dist, n)
        mean, lower, upper = calc_ci(data)
        
        # True mean for each distribution
        true_mean = 0 if dist != 'Exponential' else 1
        
        # Add CI trace
        fig.add_trace(go.Scatter(
            x=[1, 2, 3],
            y=[mean, mean, mean],
            mode='lines',
            line=dict(color='blue', width=3),
            name=f'{dist} (n={n}): Mean={mean:.3f}',
            visible=(dist == 'Normal' and n == 5)
        ))
        
        # Add CI area trace
        fig.add_trace(go.Scatter(
            x=[1, 2, 3, 3, 2, 1],
            y=[lower, lower, lower, upper, upper, upper],
            fill='toself',
            fillcolor='rgba(0, 100, 255, 0.3)',
            line=dict(color='rgba(255,255,255,0)'),
            hoverinfo='skip',
            name=f'{dist} (n={n}): CI=[{lower:.3f},{upper:.3f}]',
            visible=(dist == 'Normal' and n == 5),
            showlegend=False
        ))
        
        trace_info.append({
            'dist': dist,
            'n': n,
            'mean_idx': trace_counter,
            'ci_idx': trace_counter + 1
        })
        trace_counter += 2

# Create dropdown menu options
dropdown_buttons = []
for dist in distributions:
    for n in sample_sizes:
        # Create visibility array
        visibility = [False] * len(fig.data)
        
        # Find the indices for this combination
        for info in trace_info:
            if info['dist'] == dist and info['n'] == n:
                visibility[info['mean_idx']] = True
                visibility[info['ci_idx']] = True
                break
        
        dropdown_buttons.append(dict(
            label=f"{dist} (n={n})",
            method="update",
            args=[
                {"visible": visibility},
                {
                    "title": f"95% Confidence Interval: {dist} Distribution, n={n}",
                    "yaxis.range": [true_mean - 2, true_mean + 2] if dist != 'Exponential' else [0, 4]
                }
            ]
        ))

# Add true mean lines as shapes
shapes = []
annotations = []
for dist in distributions:
    true_mean = 0 if dist != 'Exponential' else 1
    shapes.append(dict(
        type="line",
        x0=0.5, x1=3.5,
        y0=true_mean, y1=true_mean,
        line=dict(color="red", dash="dash"),
        visible=(dist == 'Normal')
    ))
    annotations.append(dict(
        x=3.5, y=true_mean,
        text=f"True Mean = {true_mean}",
        showarrow=False,
        xanchor="left",
        font=dict(color="red"),
        visible=(dist == 'Normal')
    ))

fig.update_layout(
    title="95% Confidence Interval: Normal Distribution, n=5",
    xaxis=dict(
        visible=False,
        range=[0.5, 3.5]
    ),
    yaxis_title="Value",
    yaxis_range=[-2, 2],
    height=500,
    showlegend=True,
    legend=dict(x=0.02, y=0.98, bgcolor='rgba(255,255,255,0.8)'),
    updatemenus=[dict(
        buttons=dropdown_buttons,
        direction="down",
        x=0.02,
        y=1.15,
        xanchor="left",
        yanchor="top"
    )],
    shapes=shapes,
    annotations=annotations
)

# Update layout to manage shape/annotation visibility in dropdown
for i, button in enumerate(fig.layout.updatemenus[0].buttons):
    # Get the distribution from the button label
    dist = button.label.split(' (')[0]
    true_mean = 0 if dist != 'Exponential' else 1
    
    # Update the args to include shape and annotation visibility
    new_shapes = []
    new_annotations = []
    
    for shape in shapes:
        new_shape = shape.copy()
        new_shape['visible'] = (dist == shape.get('_dist', 'Normal'))
        new_shapes.append(new_shape)
    
    for ann in annotations:
        new_ann = ann.copy()
        new_ann['visible'] = (dist == ann.get('_dist', 'Normal'))
        new_annotations.append(new_ann)
    
    # Add yaxis range adjustment based on distribution
    y_range = [true_mean - 2, true_mean + 2] if dist != 'Exponential' else [0, 4]
    
    button.args[1].update({
        "shapes": new_shapes,
        "annotations": new_annotations,
        "yaxis.range": y_range
    })

fig.show()

```

::: {style="font-size: 50%;"}
> “The data itself is always Uniform.\
> What changes here is what we measure the average.”
:::

## To Keep in Mind

::::::: columns
:::: {.column width="50%"}
### What CLT Doesn't say

::: {style="font-size: 70%;"}

-   ❌ The population is normal

-   ❌ Individual observations are normal

-   ❌ Small samples are safe

-   ❌ Tails are well-behaved

-   CLT is not a claim about reality.
:::
::::

:::: {.column width="50%"}
### CLT as a Modeling Comfort

::: {style="font-size: 70%;"}

CLT gives us permission:

-   To model means as Gaussian

-   To compute confidence intervals

-   To quantify uncertainty

-   To reason mathematically

Even when underlying structure is different.
:::
::::
:::::::

## Gaussian Appears Everywhere

::::::: columns
:::: {.column width="50%"}
### Gaussian Is the Shape of Aggregation

::: {style="font-size: 70%;"}

-   Gaussian is stable under addition

-   Gaussian emerges from summing randomness

-   Gaussian is mathematically tractable
:::
::::

:::: {.column width="50%"}
### Reality Is Messy & Statistics Is Structured

::: {style="font-size: 70%;"}

-   Reality → messy, irregular, asymmetric

-   Statistics → summaries, averages, aggregates

-   CLT explains **why**: *Messy reality → clean statistical behavior*
:::
::::
:::::::

## When CLT Can Mislead You?

::: {style="font-size: 70%;"}

CLT breaks when:

-   Data is heavy-tailed

-   Strong dependence exists

-   Sample size is small

-   You care about extremes, not averages

**Gaussian comfort ≠ guaranteed safety.**
:::

# Thank You

## In Future Seasons we will Discuss Why?

-   Why `n=30` is a myth

-   Why CLT fails in business & ML